{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T14:12:15.784161Z",
     "iopub.status.busy": "2025-04-15T14:12:15.783481Z",
     "iopub.status.idle": "2025-04-15T14:13:08.722519Z",
     "shell.execute_reply": "2025-04-15T14:13:08.721622Z",
     "shell.execute_reply.started": "2025-04-15T14:12:15.784135Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: Tesla P100-PCIE-16GB\n",
      "GPU memory: 17.059545088 GB\n",
      "Loaded 189 training games\n",
      "Processed 13128 messages with 590 deceptive and 12538 truthful\n",
      "Processed 1416 messages with 56 deceptive and 1360 truthful\n",
      "Processed 2741 messages with 240 deceptive and 2501 truthful\n",
      "Vocabulary size: 4650\n",
      "Positive weight (truthful/deceptive): 21.2508\n",
      "\n",
      "Starting Epoch 1...\n",
      "Epoch 1 Losses - Train: 1.3315, Validation: 1.2904\n",
      "Validation F1 (threshold=0.5): 0.1083\n",
      "Best threshold: 0.5112, Best F1: 0.1220\n",
      "New best model saved with F1: 0.1220\n",
      "\n",
      "Starting Epoch 2...\n",
      "Epoch 2 Losses - Train: 1.3089, Validation: 1.2885\n",
      "Validation F1 (threshold=0.5): 0.1159\n",
      "Best threshold: 0.5021, Best F1: 0.1245\n",
      "New best model saved with F1: 0.1245\n",
      "\n",
      "Starting Epoch 3...\n",
      "Epoch 3 Losses - Train: 1.2763, Validation: 1.2829\n",
      "Validation F1 (threshold=0.5): 0.0956\n",
      "Best threshold: 0.5311, Best F1: 0.1296\n",
      "New best model saved with F1: 0.1296\n",
      "\n",
      "Starting Epoch 4...\n",
      "Epoch 4 Losses - Train: 1.2673, Validation: 1.2814\n",
      "Validation F1 (threshold=0.5): 0.0949\n",
      "Best threshold: 0.5231, Best F1: 0.1245\n",
      "No improvement for 1 epoch(s).\n",
      "\n",
      "Starting Epoch 5...\n",
      "Epoch 5 Losses - Train: 1.2272, Validation: 1.2802\n",
      "Validation F1 (threshold=0.5): 0.0946\n",
      "Best threshold: 0.5177, Best F1: 0.1133\n",
      "No improvement for 2 epoch(s).\n",
      "\n",
      "Starting Epoch 6...\n",
      "Epoch 6 Losses - Train: 1.1744, Validation: 1.2819\n",
      "Validation F1 (threshold=0.5): 0.1194\n",
      "Best threshold: 0.4995, Best F1: 0.1221\n",
      "No improvement for 3 epoch(s).\n",
      "Early stopping triggered after 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/34449727.py:518: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Performance\n",
      "Accuracy: 0.8475, F1: 0.1220, AUC: 0.6253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.87      0.92      1360\n",
      "         1.0       0.08      0.27      0.12        56\n",
      "\n",
      "    accuracy                           0.85      1416\n",
      "   macro avg       0.52      0.57      0.52      1416\n",
      "weighted avg       0.93      0.85      0.89      1416\n",
      "\n",
      "\n",
      "Test Set Performance\n",
      "Accuracy: 0.8121, F1: 0.2161, AUC: 0.6395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.86      0.89      2501\n",
      "         1.0       0.17      0.30      0.22       240\n",
      "\n",
      "    accuracy                           0.81      2741\n",
      "   macro avg       0.55      0.58      0.55      2741\n",
      "weighted avg       0.86      0.81      0.83      2741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "\n",
    "# -------------------\n",
    "# Utility Functions\n",
    "# -------------------\n",
    "def check_gpu_availability():\n",
    "    \"\"\"Check if CUDA is available and print GPU details if present.\"\"\"\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA available:\", is_cuda_available)\n",
    "    if is_cuda_available:\n",
    "        print(\"GPU device:\", torch.cuda.get_device_name(0))\n",
    "        print(\"GPU memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "    return torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "\n",
    "def diplomacy_tokenizer(text):\n",
    "    \"\"\"Tokenize text into words and punctuation, converting to lowercase.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to tokenize.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tokens (words and punctuation).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    return re.findall(r\"\\w+|[.,!?;]\", text)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load data from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries parsed from JSONL.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# -------------------\n",
    "# Vocabulary Class\n",
    "# -------------------\n",
    "class Vocab:\n",
    "    def __init__(self, min_freq=2):\n",
    "        \"\"\"Initialize vocabulary with special tokens and minimum frequency.\n",
    "        \n",
    "        Args:\n",
    "            min_freq (int): Minimum frequency for tokens to be included in vocab.\n",
    "        \"\"\"\n",
    "        self.token_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx_to_token = ['<PAD>', '<UNK>']\n",
    "        self.min_freq = min_freq\n",
    "        self.harbinger_indices = set()\n",
    "\n",
    "    def build_vocab(self, texts, harbingers):\n",
    "        \"\"\"Build vocabulary from texts, including harbinger tokens.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of text messages to build vocab from.\n",
    "            harbingers (list): List of harbinger tokens to track.\n",
    "        \"\"\"\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(diplomacy_tokenizer(text))\n",
    "        for token, freq in counter.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        self.harbinger_indices = set(self.token_to_idx.get(token, 1) for token in harbingers\n",
    "                                    if token in self.token_to_idx)\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"Encode a list of tokens into indices.\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): List of tokens to encode.\n",
    "        \n",
    "        Returns:\n",
    "            list: List of token indices, using <UNK> for unknown tokens.\n",
    "        \"\"\"\n",
    "        return [self.token_to_idx.get(t, 1) for t in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "# -------------------\n",
    "# Dataset Class\n",
    "# -------------------\n",
    "class DiplomacyDataset(Dataset):\n",
    "    def __init__(self, messages, labels, vocab, context_size=2, max_len=300):\n",
    "        \"\"\"Initialize dataset for Diplomacy messages.\n",
    "        \n",
    "        Args:\n",
    "            messages (list): List of text messages.\n",
    "            labels (list): List of binary labels (0=truthful, 1=deceptive).\n",
    "            vocab (Vocab): Vocabulary object for token encoding.\n",
    "            context_size (int): Number of previous messages to include as context.\n",
    "            max_len (int): Maximum length of token sequence.\n",
    "        \"\"\"\n",
    "        self.messages = messages\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "        self.max_len = max_len\n",
    "        self.encoded = [vocab.encode(diplomacy_tokenizer(msg)) for msg in messages]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.messages)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample with context, harbinger mask, and label.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (context_tensor, harbinger_mask_tensor, label_tensor)\n",
    "        \"\"\"\n",
    "        context = []\n",
    "        for i in range(max(0, idx - self.context_size), idx + 1):\n",
    "            context.extend(self.encoded[i])\n",
    "        if len(context) < self.max_len:\n",
    "            context += [0] * (self.max_len - len(context))\n",
    "        else:\n",
    "            context = context[:self.max_len]\n",
    "        harbinger_mask = [1 if token in self.vocab.harbinger_indices else 0 for token in context]\n",
    "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
    "        harbinger_mask_tensor = torch.tensor(harbinger_mask, dtype=torch.float)\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return context_tensor, harbinger_mask_tensor, label_tensor\n",
    "\n",
    "# -------------------\n",
    "# Model Components\n",
    "# -------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"Initialize positional encoding for transformer.\n",
    "        \n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (embedding + features).\n",
    "            max_len (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input embeddings.\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        \n",
    "        Returns:\n",
    "            tensor: Input with positional encoding added.\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=127, num_heads=4, num_layers=2, hidden_dim=256, dropout=0.3, max_len=300):\n",
    "        \"\"\"Initialize transformer-based classifier.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embed_dim (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of transformer encoder layers.\n",
    "            hidden_dim (int): Dimension of feedforward network.\n",
    "            dropout (float): Dropout rate.\n",
    "            max_len (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert (embed_dim + 1) % num_heads == 0, \"embed_dim + 1 must be divisible by num_heads\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        d_model = embed_dim + 1\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x, harb_mask):\n",
    "        \"\"\"Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x (tensor): Input token indices (batch_size, seq_len).\n",
    "            harb_mask (tensor): Harbinger mask (batch_size, seq_len).\n",
    "        \n",
    "        Returns:\n",
    "            tensor: Logits (batch_size,).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        harb_mask = harb_mask.unsqueeze(2)\n",
    "        combined = torch.cat([embedded, harb_mask], dim=2)\n",
    "        combined = self.pos_encoder(combined)\n",
    "        mask = (x == 0)\n",
    "        output = self.transformer_encoder(combined, src_key_padding_mask=mask)\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.dropout(output)\n",
    "        return self.fc(output).squeeze(1)\n",
    "\n",
    "# -------------------\n",
    "# Training and Evaluation\n",
    "# -------------------\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): Training data loader.\n",
    "        optimizer (optim.Optimizer): Optimizer.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to run computations on.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for x, harb_mask, y in dataloader:\n",
    "        x, harb_mask, y = x.to(device), harb_mask.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, harb_mask)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, dataloader, device, criterion=None):\n",
    "    \"\"\"Evaluate the model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        dataloader (DataLoader): Data loader for evaluation.\n",
    "        device (torch.device): Device to run computations on.\n",
    "        criterion (nn.Module, optional): Loss function to compute validation loss.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Arrays of logits, true labels, and average loss (if criterion provided).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_logits, all_labels, losses = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for x, harb_mask, y in dataloader:\n",
    "            x, harb_mask, y = x.to(device), harb_mask.to(device), y.to(device)\n",
    "            logits = model(x, harb_mask)\n",
    "            all_logits.extend(logits.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "            if criterion:\n",
    "                loss = criterion(logits, y)\n",
    "                losses.append(loss.item())\n",
    "    return np.array(all_logits), np.array(all_labels), np.mean(losses) if losses else None\n",
    "\n",
    "def print_metrics(true, preds, probs, name=\"\"):\n",
    "    \"\"\"Print classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        true (array): True labels.\n",
    "        preds (array): Predicted labels.\n",
    "        probs (array): Predicted probabilities.\n",
    "        name (str): Dataset name for display.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(true, preds)\n",
    "    f1 = f1_score(true, preds)\n",
    "    auc = roc_auc_score(true, probs)\n",
    "    print(f\"\\n{name} Set Performance\")\n",
    "    print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "    print(classification_report(true, preds))\n",
    "\n",
    "def find_best_threshold(logits, labels):\n",
    "    \"\"\"Find the optimal classification threshold based on F1 score.\n",
    "    \n",
    "    Args:\n",
    "        logits (array): Model logits.\n",
    "        labels (array): True labels.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Best threshold and corresponding F1 score.\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    precision, recall, thresholds = precision_recall_curve(labels, probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx], f1_scores[best_idx]\n",
    "\n",
    "def track_losses(train_loss, val_loss, epoch, train_losses, val_losses):\n",
    "    \"\"\"Track and log training and validation losses.\n",
    "    \n",
    "    Args:\n",
    "        train_loss (float): Training loss for the current epoch.\n",
    "        val_loss (float): Validation loss for the current epoch.\n",
    "        epoch (int): Current epoch number.\n",
    "        train_losses (list): List to store training losses.\n",
    "        val_losses (list): List to store validation losses.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated train_losses and val_losses lists.\n",
    "    \"\"\"\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1} Losses - Train: {train_loss:.4f}, Validation: {val_loss:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# -------------------\n",
    "# Data Preprocessing\n",
    "# -------------------\n",
    "def preprocess_messages(data):\n",
    "    \"\"\"Extract messages and labels from game data.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of game dictionaries with messages and labels.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Messages, labels, count of truthful, count of deceptive.\n",
    "    \"\"\"\n",
    "    messages, labels = [], []\n",
    "    for game in data:\n",
    "        game_messages = game.get(\"messages\", [])\n",
    "        sender_labels = game.get(\"sender_labels\", [])\n",
    "        if not game_messages or len(game_messages) < 2 or len(sender_labels) != len(game_messages):\n",
    "            continue\n",
    "        for i, msg in enumerate(game_messages):\n",
    "            is_deceptive = 0 if sender_labels[i] else 1\n",
    "            messages.append(msg)\n",
    "            labels.append(is_deceptive)\n",
    "    num_deceptive = sum(labels)\n",
    "    num_truthful = len(labels) - num_deceptive\n",
    "    print(f\"Processed {len(messages)} messages with {num_deceptive} deceptive and {num_truthful} truthful\")\n",
    "    return messages, labels, num_truthful, num_deceptive\n",
    "\n",
    "def load_datasets(data_dir):\n",
    "    \"\"\"Load training, validation, and test datasets.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing JSONL files.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Train, validation, and test data.\n",
    "    \"\"\"\n",
    "    train_data = load_jsonl(\"/kaggle/input/deception/train.jsonl\")\n",
    "    val_data = load_jsonl(\"/kaggle/input/deception/validation.jsonl\")\n",
    "    test_data = load_jsonl(\"/kaggle/input/deception/test.jsonl\")\n",
    "    print(f\"Loaded {len(train_data)} training games\")\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# -------------------\n",
    "# Training Pipeline\n",
    "# -------------------\n",
    "def setup_training_components(train_msgs, train_labels, val_msgs, val_labels, test_msgs, test_labels, harbingers, config):\n",
    "    \"\"\"Set up vocabulary, datasets, data loaders, and model.\n",
    "    \n",
    "    Args:\n",
    "        train_msgs (list): Training messages.\n",
    "        train_labels (list): Training labels.\n",
    "        val_msgs (list): Validation messages.\n",
    "        val_labels (list): Validation labels.\n",
    "        test_msgs (list): Test messages.\n",
    "        test_labels (list): Test labels.\n",
    "        harbingers (list): Harbinger tokens.\n",
    "        config (dict): Configuration dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Model, optimizer, criterion, data loaders, vocabulary.\n",
    "    \"\"\"\n",
    "    vocab = Vocab(min_freq=2)\n",
    "    vocab.build_vocab(train_msgs, harbingers)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    train_num_truthful = len(train_labels) - sum(train_labels)\n",
    "    train_num_deceptive = sum(train_labels)\n",
    "    pos_weight = train_num_truthful / train_num_deceptive if train_num_deceptive > 0 else 1.0\n",
    "    print(f\"Positive weight (truthful/deceptive): {pos_weight:.4f}\")\n",
    "\n",
    "    train_set = DiplomacyDataset(train_msgs, train_labels, vocab, config['context_size'], config['max_len'])\n",
    "    val_set = DiplomacyDataset(val_msgs, val_labels, vocab, config['context_size'], config['max_len'])\n",
    "    test_set = DiplomacyDataset(test_msgs, test_labels, vocab, config['context_size'], config['max_len'])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=config['batch_size'], num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=config['batch_size'], num_workers=2, pin_memory=True)\n",
    "\n",
    "    model = TransformerClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=config['embed_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_layers=config['num_layers'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        dropout=config['dropout'],\n",
    "        max_len=config['max_len']\n",
    "    ).to(config['device'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, dtype=torch.float).to(config['device']))\n",
    "\n",
    "    return model, optimizer, criterion, train_loader, val_loader, test_loader, vocab\n",
    "\n",
    "def run_training():\n",
    "    \"\"\"Main training function orchestrating the training pipeline.\"\"\"\n",
    "    config = {\n",
    "        'data_dir': \"/kaggle/input/diplomacy\" if os.path.exists(\"/kaggle/input\") else \".\",\n",
    "        'batch_size': 32,\n",
    "        'context_size': 2,\n",
    "        'max_len': 300,\n",
    "        'embed_dim': 127,\n",
    "        'num_heads': 4,\n",
    "        'num_layers': 2,\n",
    "        'hidden_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'lr': 1e-4,\n",
    "        'num_epochs': 15,\n",
    "        'early_stop_patience': 3,\n",
    "        'device': check_gpu_availability()\n",
    "    }\n",
    "\n",
    "    harbingers = [\n",
    "        \"maybe\", \"perhaps\", \"possibly\", \"might\", \"could\", \"would\", \"should\", \"potentially\", \"presumably\",\n",
    "        \"likely\", \"unlikely\", \"probably\", \"possibly\", \"conceivably\", \"hopefully\", \"eventually\", \"ultimately\",\n",
    "        \"definitely\", \"certainly\", \"surely\", \"absolutely\", \"undoubtedly\", \"clearly\", \"obviously\", \"apparently\",\n",
    "        \"seemingly\", \"allegedly\", \"supposedly\", \"reportedly\", \"essentially\", \"basically\", \"fundamentally\",\n",
    "        \"significantly\", \"considerably\", \"virtually\", \"practically\", \"nearly\", \"almost\", \"marginally\", \"somewhat\",\n",
    "        \"think\", \"believe\", \"feel\", \"suppose\", \"guess\", \"wonder\", \"assume\", \"suspect\", \"estimate\", \"imagine\",\n",
    "        \"figure\", \"reckon\", \"expect\", \"predict\", \"anticipate\", \"foresee\", \"presume\", \"infer\", \"deduce\", \"conclude\",\n",
    "        \"gather\", \"surmise\", \"speculate\", \"theorize\", \"hypothesize\", \"sense\", \"perceive\", \"notice\", \"realize\",\n",
    "        \"recognize\", \"understand\", \"know\", \"remember\", \"recall\", \"forget\",\n",
    "        \"sort\", \"kind\", \"rather\", \"quite\", \"somewhat\", \"slightly\", \"moderately\", \"relatively\", \"comparatively\",\n",
    "        \"reasonably\", \"fairly\", \"pretty\", \"mostly\", \"mainly\", \"primarily\", \"partially\", \"largely\", \"substantially\",\n",
    "        \"typically\", \"generally\", \"usually\", \"normally\", \"commonly\", \"regularly\", \"often\", \"frequently\", \"sometimes\",\n",
    "        \"occasionally\", \"rarely\", \"seldom\",\n",
    "        \"very\", \"really\", \"extremely\", \"absolutely\", \"totally\", \"completely\", \"utterly\", \"perfectly\", \"entirely\",\n",
    "        \"thoroughly\", \"fully\", \"wholly\", \"downright\", \"positively\", \"simply\", \"just\", \"merely\", \"only\", \"literally\",\n",
    "        \"actually\", \"honestly\", \"truly\", \"genuinely\", \"sincerely\", \"frankly\",\n",
    "        \"about\", \"around\", \"approximately\", \"roughly\", \"nearly\", \"close to\", \"in the range of\", \"something like\",\n",
    "        \"or so\", \"more or less\", \"give or take\", \"in general\", \"on the whole\", \"all things considered\", \"by and large\",\n",
    "        \"for the most part\", \"to some extent\", \"in some ways\", \"in a sense\", \"in theory\", \"technically\",\n",
    "        \"strictly speaking\", \"officially\", \"formally\", \"nominally\", \"effectively\", \"in effect\", \"in principle\",\n",
    "        \"ideally\", \"theoretically\",\n",
    "        \"now\", \"currently\", \"presently\", \"at present\", \"at the moment\", \"these days\", \"lately\", \"recently\",\n",
    "        \"in recent times\", \"over time\", \"with time\", \"in time\", \"sooner or later\", \"eventually\", \"ultimately\",\n",
    "        \"in the end\", \"at the end of the day\", \"when all is said and done\", \"in the long run\", \"in the final analysis\",\n",
    "        \"according to\", \"as per\", \"based on\", \"in light of\", \"in view of\", \"given that\", \"seeing as\", \"considering\",\n",
    "        \"taking into account\", \"from what I understand\", \"from what I gather\", \"from my perspective\", \"in my opinion\",\n",
    "        \"to my knowledge\", \"as far as I know\",\n",
    "        \"diplomatically\", \"strategically\", \"tactically\", \"politically\", \"negotiable\", \"flexible\", \"adaptable\",\n",
    "        \"revisable\", \"amendable\", \"subject to\", \"conditional upon\", \"dependent on\", \"contingent on\", \"pending\",\n",
    "        \"awaiting\", \"considering\", \"reviewing\", \"evaluating\", \"assessing\", \"monitoring\", \"observing\", \"watching\",\n",
    "        \"tracking\", \"following\", \"pursuant to\"\n",
    "    ]\n",
    "\n",
    "    train_data, val_data, test_data = load_datasets(config['data_dir'])\n",
    "    train_msgs, train_labels, _, _ = preprocess_messages(train_data)\n",
    "    val_msgs, val_labels, _, _ = preprocess_messages(val_data)\n",
    "    test_msgs, test_labels, _, _ = preprocess_messages(test_data)\n",
    "\n",
    "    model, optimizer, criterion, train_loader, val_loader, test_loader, vocab = setup_training_components(\n",
    "        train_msgs, train_labels, val_msgs, val_labels, test_msgs, test_labels, harbingers, config\n",
    "    )\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_path = os.path.join(\"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\", \"best_diplomacy_model.pth\")\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nStarting Epoch {epoch+1}...\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, config['device'])\n",
    "\n",
    "        val_logits, val_true, val_loss = evaluate(model, val_loader, config['device'], criterion)\n",
    "        train_losses, val_losses = track_losses(train_loss, val_loss, epoch, train_losses, val_losses)\n",
    "\n",
    "        val_probs = torch.sigmoid(torch.tensor(val_logits)).numpy()\n",
    "        val_preds = (val_probs > 0.5).astype(int)\n",
    "        val_f1 = f1_score(val_true, val_preds)\n",
    "        print(f\"Validation F1 (threshold=0.5): {val_f1:.4f}\")\n",
    "\n",
    "        current_threshold, current_f1 = find_best_threshold(val_logits, val_true)\n",
    "        print(f\"Best threshold: {current_threshold:.4f}, Best F1: {current_f1:.4f}\")\n",
    "\n",
    "        if current_f1 > best_val_f1:\n",
    "            best_val_f1 = current_f1\n",
    "            best_threshold = current_threshold\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with F1: {current_f1:.4f}\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "            if epochs_without_improvement >= config['early_stop_patience']:\n",
    "                print(f\"Early stopping triggered after {config['early_stop_patience']} epochs.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    for name, loader, labels in [(\"Validation\", val_loader, val_labels), (\"Test\", test_loader, test_labels)]:\n",
    "        logits, true, _ = evaluate(model, loader, config['device'])\n",
    "        probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "        preds = (probs > best_threshold).astype(int)\n",
    "        print_metrics(true, preds, probs, name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7135202,
     "sourceId": 11393113,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
